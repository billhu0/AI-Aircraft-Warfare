\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
% \usepackage{ctex}
\usepackage{graphicx}   
\usepackage{wrapfig}
\usepackage{float}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{multicol}

\bibliographystyle{unsrt}

\title{Playing Aircraft Warfare Game with Reinforcement Learning}

\author{
    Yan Zeng\\
    ShanghaiTech University\\
    \texttt{zengyan@shanghaitech.edu.cn}\\
    \AND
    Yijie Fan\\
    ShanghaiTech University\\
    \texttt{fanyj@shanghaitech.edu.cn}\\
    \AND
    Luojia Hu\\
    ShanghaiTech University\\
    \texttt{hulj@shanghaitech.edu.cn}\\
    \AND
    Ziang Li\\
    ShanghaiTech University\\
    \texttt{liza1@shanghaitech.edu.cn}\\
    \AND
    Chongyu Wang\\
    ShanghaiTech University\\
    \texttt{wangchy5@shanghaitech.edu.cn}\\    
}


\begin{document}


\maketitle

% 摘要 分数占比10%s
\begin{abstract}
    摘要没写
\end{abstract}


\section{Introduction}
% @inproceedings{kempka2016vizdoom,
% title={Vizdoom: A doom-based ai research platform for visual reinforcement learning},
% author={Kempka, Micha{\l} and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Ja{\'s}kowski, Wojciech},
% booktitle={2016 IEEE conference on computational intelligence and games (CIG)},
% pages={1--8},
% year={2016},
% organization={IEEE}
% }


% @article{heess2015learning,
% title={Learning continuous control policies by stochastic value gradients},
% author={Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
% journal={Advances in neural information processing systems},
% volume={28},
% year={2015}
% }

%     @article{mnih2013playing,
%     title={Playing atari with deep reinforcement learning},
%     author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
%     journal={arXiv preprint arXiv:1312.5602},
%     year={2013}
%   }   

    % \subsection{Motivation}

    % 《经典飞机大战》是腾讯交流软件微信5.0版本在2013年8月推出的软件内置经典小游戏。
    % 该游戏的玩法十分简单，玩家只需要通过键盘上的上下左右键控制飞机移动，通过空格键发射子弹，击毁敌方飞机即可获得分数。
    % 在游戏中，玩家可以通过不断击毁敌方飞机获得分数，同时也会不断遭受敌方飞机的攻击，当玩家的血量为0时游戏结束。
    % 该游戏的玩法十分简单，但是玩家在游戏中的表现却十分复杂，这是因为游戏中的飞机是有自己的智能的，它们会根据玩家的行为做出反应。
    % 例如，当玩家的飞机靠近敌方飞机时，敌方飞机会主动向玩家的飞机发起攻击
    % 在这个project中，我们将这个游戏作为我们的仿真环境，通过强化学习的方法来训练飞机，使得飞机能够自主地在游戏中进行战斗。

\begin{wrapfigure}{H}{7cm}
    \centering
    \includegraphics[width=\linewidth]{pictures/game.jpg}
    \caption{The Aircraft Warfare Game}
    \label{fig:aircraft_warfare_game}
\end{wrapfigure}

\par Aircraft Warfare is a classic game we all enjoyed very much, which is also perfect for fully practicing what we have learned in class, as shown in Figure \ref{fig:aircraft_warfare_game}. The rule of the game is rather simple. The \textbf{goal} of the player -- a upward facing aircraft plane -- is to make the score as high as possible. The player can get \textbf{reward} by managing to hit enemies -- downward facing aircraft planes -- with five \textbf{actions}, namely, up, down, left, right, and using the bomb.  The \textbf{state} includes the life value and positions of player and enemies and so on. Game overs when life value decreases to 0.

\par However, playing the game well is quit tough when it comes to difficult mode. Hence we turn to AI for help. To the best of our knowledge, reinforcement learning has shown to be very successful in mastering control policies in lots of tasks such as object recognition and solving physics-based control problems\cite{}. Specially, Deep Q-Networks (DQN) are proved to be effective in playing games and even could defeat top human Go players\cite{}. The reason they can work well is that games can quickly generate large amounts of naturally self-annotated (state-action-reward) data, which are high-quality training material for reinforcement learning. That is, this property of games allows deep reinforcement learning to obtain a large number of samples for training to enhance the effect almost costlessly. For example, DeepMind's achievements such as playing Atari with DQN, AlphaGo defeating Go masters, AlphaZero becoming a self-taught master of Go and Chess. And OpenAI's main research is based on games, such as the progress made in Dota. For these reasons, we believe that training agents based on deep reinforcement learning techniques is a promising solution

\par In this paper, we implement an AI-agent for playing the Aircraft Warfare with Approximate Q-learning method and Deep Q-learning method. For Approximate Q-learning, we extracted four most useful features, i.e., interactions with the closest aircraft, bomb supply, double bullet, and the trade-off between movement and explosion. We trained our agent with an online learning method. For Deep Q-learning, we utilize a convolutional neural network which takes the screen patch as input and outputs the expected converged sum of the discounted reward of taking each action given the current state of the 6 legal actions. We also trained two DQN in case overfitting. 

\par result...

% 未完待续
    % In brief, players need to control the plane movement through the up, down, left and right keys on the keyboard, or shoot bullets to destroy the enemy aircrafts (downward facing aircrafts) to obtain points. 

    % Players can obtain points by destroying enemy aircrafts, but they may also be attacked by enemies. When the player's blood volume is 0, the game ends.
    % However, the performance of the player in the game is very complex. Because the aircraft in the game are intelligent, and they will react according to the player's behavior.
    % For example, when the player's plane is close to the enemy plane, the enemy plane will actively launch an attack on the player's plane.

    % In this project, we use this game as our simulation environment, and train the aircraft through the reinforcement learning method to make the aircraft autonomous in the game.

    % \subsection{Aircraft Warfare game settings} 

    % 在这个project中，我们使用了《经典飞机大战》的游戏规则，但是我们对游戏的设置进行了一些修改，以适应我们的强化学习算法。我们采用pygame库来实现游戏的界面，游戏的界面如图\ref{fig:aircraft_warfare_game}所示。

    % In this project, we use the original rules of the `Aircraft Warfare' game, but we make some changes to the game settings to adapt to our reinforcement learning algorithm. We use the pygame library to implement the game interface, as shown in Figure \ref{fig:aircraft_warfare_game}. 

    % % 在游戏中，玩家的飞机会持续发射子弹，玩家可以通过键盘上的上下左右键控制飞机移动。玩家通过空格键发射全屏子弹，能够清空屏幕中出现的所有敌机。但全屏子弹的数量具有限制。

    % In the game, the player's plane will continue to fire bullets, and the player can control the plane movement through the up, down, left and right keys on the keyboard. The player can fire a full-screen bullet through the space key, which can clear all the enemy planes that appear on the screen. But the number of full-screen bullets is limited.

    % % 玩家需要打中敌机来获得分数。

    % % 敌机有三种类型，分别是小型敌机、中型敌机和大型敌机。小型飞机没有血量，打中一次即可消灭。中型和大型飞机都有一定的血量，打中一次会减少一定的血量，当血量为0时消灭。同时，游戏中会有血包和double bullet出现，玩家可以通过接触血包来增加生命数量，通过接触double bullet可以在接下来的一段时间内发射攻击范围更大的双倍子弹。

    % The player needs to hit the enemy plane to get points.

    % There are three types of enemy planes, namely small, medium and large enemy planes. Small enemies can be destroyed by a single bullet hitting. Medium and large enemies have a certain blood volume that hitting once will reduce it, and the enemy will be destroyed when the blood volume is 0. 
    
    % At the same time, there will be blood bags and double bullets appearing in the game. The player can increase the number of lives by touching the blood bag, and can fire double bullets with a larger attack range in the next period of time by touching the double bullet.

\section{ Methodology}
    \subsection{Approximate Q-learning}
        Since the number of states in the Aircraft Warfare Game is extremely large, we choose Approximate Q-learning for reinforcement learning. 
    \subsubsection{Feature Extraction}
        In total, we extracted four features, which are the interaction with the closest aircraft, the interaction with bomb supply, the interaction with double bullet , and the trade-off between movement and explosion. The following will explain in detail how the features are extracted.
        \[Q(s,a) = w_1f_1(s,a)+w_2f_2(s,a)+w_3f_3(s,a)+w_4f_4(s,a)\]
    
        In this game, it is very necessary to control the distance between our aircraft and enemy aircraft and props.Thus,we use the Manhattan distance to measure the interaction of the aircraft with the external environment. For the current state $s$, we observe the location of the nearest enemies, game props to the aircraft. Next, we analyze the effect of action $a$ on the Manhattan distance between the aircraft and the aforementioned target. The weights corresponding to each of these features reflect the choices and trade-offs made by the aircraft in various situations.The purpose of the plane's actions can be highly summarized as attacking, dodging, and picking up props. Both affect the value of $Q(s,a)$ and the training of Approximate Q-learning.
    \subsubsection{Training}
        In the training phase， we train the learning with multiple hyperparameters and update the weights according to the formula below.
        \[ \mathrm{difference} = r+\gamma \mathop{\mathrm{max}}_{\alpha'}Q(s',a')-Q(s,a)\]
        \[ Q(s,a) \xleftarrow{} Q(s,a) +\alpha[\mathrm{difference}]\]
        \[w_i \xleftarrow{} w_i+\alpha[\mathrm{difference}] f_i(s,a)\]
        Our training is an online learning approach. After getting a new state $s$ in each round, we choose the action $a$ with the highest $Q(s,a)$. After taking the action in the current round, we get state $s$. Then, we use these two states with association to obtain the difference and update the parameters of the model. $\gamma$ is the discount parameter.$\alpha$ is the learning rate. The adjustment of these two parameters can better help our model to converge.
        
        % % 在这个project中，我们会使用approximate q-learning 来训练aircraft玩游戏。
    
        % We will use an approximate Q-learning to train the aircraft to play the game. The approximate Q-learning algorithm is a model-free reinforcement learning algorithm, which uses a weighted sum of features to approximate the Q function. It is suitable when the state space is large. 
    
        % \subsection{Features and Reward function}
    
        % The features consists of mainly the position of the aircraft, enemies, and bouns (double bullet and additional life). In addition, the current status including the score, the life number remaining are also used. We will take a weighted sum of these features as the current state.
    
        % % \subsection{Reward function}
    
        % \subsection{Action}
    \subsection{Deep Q Network}
    % DQN属于DRL（深度强化学习）的一种，它是深度学习与Q学习的结合体。前面讲了采用S-A表格的局限性，当状态和行为的组合不可穷尽时，就无法通过查表的方式选取最优的Action了。这时候就该想到深度学习了，想通过深度学习找到最优解在很多情况下确实不太靠谱，但是找到一个无限逼近最优解的次优解，倒是没有问题的。
    % 因此DQN实际上，总体思路还是用的Q学习的思路，不过对于给定状态选取哪个动作所能得到的Q值，却是由一个深度神经网络来计算的了
    Deep Q Network is a kind of Deep Reinforcement Learning, which is a combination of Deep Learning and Q-learning. Due to the limitations of Q-learning that it is impossible to choose the best action when the number of combinations of states and actions is infinite, using a deep neural network to help determine the action is reasonable. 
    
    \subsubsection{DQN algorithm} In the Aircraft Warfare Game the environment is deterministic, so all the equations listed below are formulated deterministically for simplicity. Our aim will be to train a policy that can maximize the cumulative, discounted reward $R_{t_0} = \sum_{t=t_0}^{\infty} {\gamma}^{t-t_0}r_t$,here the $\gamma$ is the discount factor,which should be a constant between 0 and 1 to make sure the sum can converge.In the Q-learning algorithm, we get a table of the $Q$ values of the combinations of states and actions, then we construct a policy that maximizes the rewards:
    $$\pi^*(s) = \arg\!\max_a \ Q^*(s, a)$$
    However, since the number of the combinations of states and actions is infinite in this scene, so we use a neural network to resemble $Q^*$. And by Bellman equation,we get:
    $$Q^{\pi}(s, a) = r + \gamma Q^{\pi}(s', \pi(s'))$$
    The difference between the two sides of the equation is known as the difference discussed in the lecture:
    $$\delta = Q(s, a) - (r + \gamma \max_{a'} Q(s', a'))$$
    To minimize this difference, we use the Huber loss which acts
    like the mean squared error when the error is small, but like the mean
    absolute error when the error is large - this makes it more robust to
    outliers when the estimates of $Q$ are very noisy. We calculate
    this over a batch of transitions, $B$, sampled from the replay
    memory:
    $$\mathcal{L} = \frac{1}{|B|}\sum_{(s, a, s', r) \ \in \ B} \mathcal{L}(\delta)$$
    
    $$\text{where} \quad \mathcal{L}(\delta) = \begin{cases}
         \frac{1}{2}{\delta^2}  & \text{for } |\delta| \le 1, \\
         |\delta| - \frac{1}{2} & \text{otherwise.}
       \end{cases}$$
    \subsubsection{DQN Structure and Training}
    Our network is a convolutional neural network which takes the screen patch as input and outputs the expected converged sum of the discounted reward of taking each action given the current state of the 6 legal actions. To prevent overfitting, we trained two Deep Q Networks: policy network and target network. They have the same structure but the parameters are different. We get the best action from the policy network and computes the $\max_{a'} Q(s', a')$ from the target network for added stability. How can this method help prevent overfitting? Suppose there is only one network, when the parameters are updated, not only does the $Q(s, a)$ change, but the $\max_{a'} Q(s', a')$ also changes then the loss we want to minimize is always changing. By introducing the target network we can temporarily fix $\max_{a'} Q(s', a')$ which makes the loss a fixed value to help prevent overfitting. To explore the environment, we use the $\epsilon$ greedy method when choosing the action and the value of $\epsilon$ is decayed with time to lower the regret.
\section{Result}

\appendix



\end{document}